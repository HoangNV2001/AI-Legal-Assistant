# configs/legal_multi_agent.yml
general:
  use_uvloop: true

embedders:
  llama-3.2-nv-embedqa-1b-v2:
    _type: nim
    base_url: "http://localhost:8016/v1"
    model_name: "nvidia/llama-3.2-nv-embedqa-1b-v2"  # NIM-native; NAT sets input_type for queries

retrievers:
  reg_retriever:
    _type: milvus_retriever
    uri: http://localhost:19530
    collection_name: "vn_regulations"
    embedding_model: llama-3.2-nv-embedqa-1b-v2
    top_k: 5

    vector_field_name: vector
    text_field_name: text
    metadata_fields: [issued_date]
    search_params:
      metric_type: COSINE
      params: { "nprobe": 32 }

functions:
  # --- RAG from Milvus ---
  reg_tool:
    _type: nat_retriever
    retriever: reg_retriever
    topic: Retrieve Vietnamese regulatory documents (decrees, circulars)

  # --- Web search (Tavily) ---
  web_search_tool:
    _type: tavily_internet_search
    max_results: 8

  # --- Router / Quality-check: sub-agent (react_agent) ---
  answer_quality_check:
    _type: react_agent
    llm_name: vllm_llm
    verbose: false
    tool_names: [current_time]
    additional_instructions: |
      Role: Strict JSON scorer for legal answers.
      Input: a one-line JSON string with fields: query, draft, and signals (milvus_hits, web_hits).
      Policy: keep source-neutral. Judge groundedness/completeness by (i) primary authority (official statutes/gov portals), (ii) recency/effective dates, (iii) specificity (articles, thresholds).
      Task: evaluate groundedness, completeness, recency; choose one of: accept, need_web, need_more_rag.
      Output: return ONLY a compact one-line JSON with keys groundedness, completeness, recency_ok, action, notes.
      Do NOT print Thought/Action/Final Answer. Do NOT call any tool. No prose, no code fences.

  # --- Aggregate/Merge context: sub-agent (react_agent) ---
  aggregate_context_llm:
    _type: react_agent
    llm_name: vllm_llm
    verbose: false
    tool_names: [current_time]
    additional_instructions: |
      Role: Context merger for balanced RAG (web + Milvus).
      Priority: no fixed source priority. Resolve conflicts by (1) primary authority (official legal texts/official portals), (2) recency/effective dates, then (3) specificity.
      Input: a one-line JSON string with fields: query, milvus_snippets (list of {{text}}), web_snippets (list of {{text}}), max_tokens (int).
      Task: deduplicate; keep the most authoritative and current passages first; include secondary passages only if they add missing concrete elements (thresholds, article numbers, effective dates).
      Output: return ONLY a compact one-line JSON with key merged (list of {{text}}).
      Do NOT print Thought/Action/Final Answer. Do NOT call any tool. No prose, no code fences.

llms:
  vllm_llm:
    _type: openai
    api_key: "EMPTY"
    base_url: "http://localhost:8015/v1"
    model_name: nvidia/NVIDIA-Nemotron-Nano-9B-v2
    max_tokens: 512
    temperature: 0

workflow:
  _type: react_agent
  llm_name: vllm_llm
  verbose: true
  parse_agent_response_max_retries: 5
  tool_names:
    - web_search_tool
    - reg_tool
    - answer_quality_check
    - aggregate_context_llm
  additional_instructions: |
    /no_think
    Source policy: BALANCED (web and Milvus). No fixed preference; decide by authority, recency, and specificity.
    Scope: Decrees and Circulars only. No speculation.
    Style: Respond in Vietnamese, concise and structured; always end with: Đây không phải tư vấn pháp lý.

    STRICT ReAct FORMAT (no extra text before the first line):
    - When you NEED TO CALL A TOOL, return EXACTLY 3 lines:
      Thought: LOOKUP
      Action: <web_search_tool | reg_tool | answer_quality_check | aggregate_context_llm>
      Action Input: <valid ONE-LINE JSON, no code fences, no newlines>

      Example (web):
      Thought: LOOKUP
      Action: web_search_tool
      Action Input: {{"query":"mức phạt vượt đèn đỏ"}}

      Example (milvus):
      Thought: LOOKUP
      Action: reg_tool
      Action Input: {{"query":"xử phạt theo Nghị định 100/2019/NĐ-CP, Điều 6"}}

    - When you PRODUCE THE FINAL ANSWER, return EXACTLY 1 line:
      Final Answer: <câu trả lời tiếng Việt, kết thúc bằng 'Đây không phải tư vấn pháp lý.'>

    ACTION POLICY (balanced start):
    - If the query includes clear legal identifiers (e.g., decree/circular numbers, article/ khoản), start with reg_tool.
    - If the query is broad, policy-like, or likely impacted by latest amendments, start with web_search_tool focusing on official VN sources.
    - If evidence is incomplete after the first tool, call the other tool to fill gaps.
    - Use answer_quality_check to decide whether to refine search (need_web or need_more_rag).
    - Before the Final Answer, call aggregate_context_llm with one-line JSON containing query, milvus_snippets, web_snippets, max_tokens=6000.

eval:
  general:
    output_dir: sizing_output/eval
    dataset:
      _type: json
      file_path: ./data/eval/Legal_agent.json
